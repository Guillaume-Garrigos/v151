---
title: " Gap-Dependent Unsupervised Exploration for Reinforcement Learning "
abstract: " For the problem of task-agnostic reinforcement learning (RL), an agent
  first collects samples from an unknown environment without the supervision of reward
  signals, then is revealed with a reward and is asked to compute a corresponding
  near-optimal policy. Existing approaches mainly concern the worst-case scenarios,
  in which no structural information of the reward/transition-dynamics is utilized.
  Therefore the best sample upper bound is $\\propto\\widetilde{\\mathcal{O}}(1/\\epsilon^2)$,
  where $\\epsilon>0$ is the target accuracy of the obtained policy, and can be overly
  pessimistic. To tackle this issue, we provide an efficient algorithm that utilizes
  a gap parameter, $\\rho>0$, to reduce the amount of exploration. In particular,
  for an unknown finite-horizon Markov decision process, the algorithm takes only
  $\\widetilde{\\mathcal{O}} (1/\\epsilon \\cdot (H^3SA / \\rho + H^4 S^2 A) )$ episodes
  of exploration, and is able to obtain an $\\epsilon$-optimal policy for a post-revealed
  reward with sub-optimality gap at least $\\rho$, where $S$ is the number of states,
  $A$ is the number of actions, and $H$ is the length of the horizon, obtaining a
  nearly quadratic saving in terms of $\\epsilon$. We show that, information-theoretically,
  this bound is nearly tight for $\\rho < \\Theta(1/(HS))$ and $H>1$. We further show
  that $\\propto\\widetilde{\\mathcal{O}}(1)$ sample bound is possible for $H=1$ (i.e.,
  multi-armed bandit) or with a sampling simulator, establishing a stark separation
  between those settings and the RL setting. "
software: " https://github.com/uuujf/GapExploration "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wu22b
month: 0
tex_title: " Gap-Dependent Unsupervised Exploration for Reinforcement Learning "
firstpage: 4109
lastpage: 4131
page: 4109-4131
order: 4109
cycles: false
bibtex_author: Wu, Jingfeng and Braverman, Vladimir and Yang, Lin
author:
- given: Jingfeng
  family: Wu
- given: Vladimir
  family: Braverman
- given: Lin
  family: Yang
date: 2022-05-03
address:
container-title: Proceedings of The 25th International Conference on Artificial Intelligence
  and Statistics
volume: '151'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 3
pdf: https://proceedings.mlr.press/v151/wu22b/wu22b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
