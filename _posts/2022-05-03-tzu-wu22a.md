---
title: " Deep Layer-wise Networks Have Closed-Form Weights "
abstract: ' There is currently a debate within the neuroscience community over the
  likelihood of the brain performing backpropagation (BP). To better mimic the brain,
  training a network one layer at a time with only a "single forward pass" has been
  proposed as an alternative to bypass BP; we refer to these networks as "layer-wise"
  networks. We continue the work on layer-wise networks by answering two outstanding
  questions. First, do they have a closed-form solution? Second, how do we know when
  to stop adding more layers? This work proves that the "Kernel Mean Embedding" is
  the closed-form solution that achieves the network global optimum while driving
  these networks to converge towards a highly desirable kernel for classification;
  we call it the Neural Indicator Kernel. '
software: " https://github.com/endsley "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tzu-wu22a
month: 0
tex_title: " Deep Layer-wise Networks Have Closed-Form Weights "
firstpage: 188
lastpage: 225
page: 188-225
order: 188
cycles: false
bibtex_author: Tzu Wu, Chieh and Masoomi, Aria and Gretton, Arthur and Dy, Jennifer
author:
- given: Chieh
  family: Tzu Wu
- given: Aria
  family: Masoomi
- given: Arthur
  family: Gretton
- given: Jennifer
  family: Dy
date: 2022-05-03
address:
container-title: Proceedings of The 25th International Conference on Artificial Intelligence
  and Statistics
volume: '151'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 3
pdf: https://proceedings.mlr.press/v151/tzu-wu22a/tzu-wu22a.pdf
extras:
- label: Supplementary ZIP
  link: https://proceedings.mlr.press/v151/tzu-wu22a/tzu-wu22a-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
