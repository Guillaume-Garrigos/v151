---
title: " LocoProp: Enhancing BackProp via Local Loss Optimization "
abstract: " Second-order methods have shown state-of-the-art performance for optimizing
  deep neural networks. Nonetheless, their large memory requirement and high computational
  complexity, compared to first-order methods, hinder their versatility in a typical
  low-budget setup. This paper introduces a general framework of layerwise loss construction
  for multilayer neural networks that achieves a performance closer to second-order
  methods while utilizing first-order optimizers only. Our methodology lies upon a
  three-component loss, target, and regularizer combination, for which altering each
  component results in a new update rule. We provide examples using squared loss and
  layerwise Bregman divergences induced by the convex integral functions of various
  transfer functions. Our experiments on benchmark models and datasets validate the
  efficacy of our new approach, reducing the gap between first-order and second-order
  optimizers. "
software: " https://github.com/google-research/google-research/tree/master/locoprop "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: amid22a
month: 0
tex_title: " LocoProp: Enhancing BackProp via Local Loss Optimization "
firstpage: 9626
lastpage: 9642
page: 9626-9642
order: 9626
cycles: false
bibtex_author: Amid, Ehsan and Anil, Rohan and Warmuth, Manfred
author:
- given: Ehsan
  family: Amid
- given: Rohan
  family: Anil
- given: Manfred
  family: Warmuth
date: 2022-05-03
address:
container-title: Proceedings of The 25th International Conference on Artificial Intelligence
  and Statistics
volume: '151'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 3
pdf: https://proceedings.mlr.press/v151/amid22a/amid22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
