---
title: " Hierarchical Bayesian Bandits "
abstract: " Meta-, multi-task, and federated learning can be all viewed as solving
  similar tasks, drawn from a distribution that reflects task similarities. We provide
  a unified view of all these problems, as learning to act in a hierarchical Bayesian
  bandit. We propose and analyze a natural hierarchical Thompson sampling algorithm
  (HierTS) for this class of problems. Our regret bounds hold for many variants of
  the problems, including when the tasks are solved sequentially or in parallel; and
  show that the regret decreases with a more informative prior. Our proofs rely on
  a novel total variance decomposition that can be applied beyond our models. Our
  theory is complemented by experiments, which show that the hierarchy helps with
  knowledge sharing among the tasks. This confirms that hierarchical Bayesian bandits
  are a universal and statistically-efficient tool for learning to act with similar
  bandit tasks. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hong22c
month: 0
tex_title: " Hierarchical Bayesian Bandits "
firstpage: 7724
lastpage: 7741
page: 7724-7741
order: 7724
cycles: false
bibtex_author: Hong, Joey and Kveton, Branislav and Zaheer, Manzil and Ghavamzadeh,
  Mohammad
author:
- given: Joey
  family: Hong
- given: Branislav
  family: Kveton
- given: Manzil
  family: Zaheer
- given: Mohammad
  family: Ghavamzadeh
date: 2022-05-03
address:
container-title: Proceedings of The 25th International Conference on Artificial Intelligence
  and Statistics
volume: '151'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 3
pdf: https://proceedings.mlr.press/v151/hong22c/hong22c.pdf
extras:
- label: Supplementary ZIP
  link: https://proceedings.mlr.press/v151/hong22c/hong22c-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
