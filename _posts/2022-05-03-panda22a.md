---
title: " SparseFed: Mitigating Model Poisoning Attacks in Federated Learning with
  Sparsification "
abstract: ' Federated learning is inherently vulnerable to model poisoning attacks
  because its decentralized nature allows attackers to participate with compromised
  devices. In model poisoning attacks, the attacker reduces the modelâ€™s performance
  on targeted sub-tasks (e.g. classifying planes as birds) by uploading "poisoned"
  updates. In this paper we introduce SparseFed, a novel defense that uses global
  top-k update sparsification and device-level gradient clipping to mitigate model
  poisoning attacks. We propose a theoretical framework for analyzing the robustness
  of defenses against poisoning attacks, and provide robustness and convergence analysis
  of our algorithm. To validate its empirical efficacy we conduct an open-source evaluation
  at scale across multiple benchmark datasets for computer vision and federated learning. '
software: " https://github.com/kiddyboots216/CommEfficient/tree/attacks "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: panda22a
month: 0
tex_title: " SparseFed: Mitigating Model Poisoning Attacks in Federated Learning with
  Sparsification "
firstpage: 7587
lastpage: 7624
page: 7587-7624
order: 7587
cycles: false
bibtex_author: Panda, Ashwinee and Mahloujifar, Saeed and Nitin Bhagoji, Arjun and
  Chakraborty, Supriyo and Mittal, Prateek
author:
- given: Ashwinee
  family: Panda
- given: Saeed
  family: Mahloujifar
- given: Arjun
  family: Nitin Bhagoji
- given: Supriyo
  family: Chakraborty
- given: Prateek
  family: Mittal
date: 2022-05-03
address:
container-title: Proceedings of The 25th International Conference on Artificial Intelligence
  and Statistics
volume: '151'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 3
pdf: https://proceedings.mlr.press/v151/panda22a/panda22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
