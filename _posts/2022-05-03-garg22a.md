---
title: " How and When Random Feedback Works: A Case Study of Low-Rank Matrix Factorization "
abstract: " The success of gradient descent in ML and especially for learning neural
  networks is remarkable and robust. In the context of how the brain learns, one aspect
  of gradient descent that appears biologically difficult to realize (if not implausible)
  is that its updates rely on feedback from later layers to earlier layers through
  the same connections. Such bidirected links are relatively few in brain networks,
  and even when reciprocal connections exist, they may not be equi-weighted. Random
  Feedback Alignment (Lillicrap et al., 2016), where the backward weights are random
  and fixed, has been proposed as a bio-plausible alternative and found to be effective
  empirically. We investigate how and when feedback alignment (FA) works, focusing
  on one of the most basic problems with layered structure $n\\times m$, the goal
  is to find a low rank factorization $Z_{n \\times r}W_{r \\times m}$ that minimizes
  the error $\\|ZW-Y\\|_F$. Gradient descent solves this problem optimally. We show
  that FA finds the optimal solution when $r\\ge \\mbox{rank}(Y)$. We also shed light
  on how FA works. It is observed empirically that the forward weight matrices and
  (random) feedback matrices come closer during FA updates. Our analysis rigorously
  derives this phenomenon and shows how it facilitates convergence of FA*, a closely
  related variant of FA. We also show that FA can be far from optimal when $r < \\mbox{rank}(Y)$.
  This is the first provable separation result between gradient descent and FA. Moreover,
  the representations found by gradient descent and FA can be almost orthogonal even
  when their error $\\|ZW-Y\\|_F$ is approximately equal. As a corollary, these results
  also hold for training two-layer linear neural networks when the training input
  is isotropic, and the output is a linear function of the input. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: garg22a
month: 0
tex_title: " How and When Random Feedback Works: A Case Study of Low-Rank Matrix Factorization "
firstpage: 4070
lastpage: 4108
page: 4070-4108
order: 4070
cycles: false
bibtex_author: Garg, Shivam and Vempala, Santosh
author:
- given: Shivam
  family: Garg
- given: Santosh
  family: Vempala
date: 2022-05-03
address:
container-title: Proceedings of The 25th International Conference on Artificial Intelligence
  and Statistics
volume: '151'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 3
pdf: https://proceedings.mlr.press/v151/garg22a/garg22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
