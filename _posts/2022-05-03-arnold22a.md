---
title: " Policy Learning and Evaluation with Randomized Quasi-Monte Carlo "
abstract: " Hard integrals arise frequently in reinforcement learning, for example
  when computing expectations in policy evaluation and policy iteration. They are
  often analytically intractable and typically estimated with Monte Carlo methods,
  whose sampling contributes to high variance in policy values and gradients. In this
  work, we propose to replace Monte Carlo samples with low-discrepancy point sets.
  We combine policy gradient methods with Randomized Quasi-Monte Carlo, yielding variance-reduced
  formulations of policy gradient and actor-critic algorithms. These formulations
  are effective for policy evaluation and policy improvement, as they outperform state-of-the-art
  algorithms on standardized continuous control benchmarks. Our empirical analyses
  validate the intuition that replacing Monte Carlo with Quasi-Monte Carlo yields
  significantly more accurate gradient estimates. "
software: " https://github.com/seba-1511/qrl "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: arnold22a
month: 0
tex_title: " Policy Learning and Evaluation with Randomized Quasi-Monte Carlo "
firstpage: 1041
lastpage: 1061
page: 1041-1061
order: 1041
cycles: false
bibtex_author: Arnold, S\'ebastien M. R. and L'Ecuyer, Pierre and Chen, Liyu and Chen,
  Yi-Fan and Sha, Fei
author:
- given: Sébastien M. R.
  family: Arnold
- given: Pierre
  family: L’Ecuyer
- given: Liyu
  family: Chen
- given: Yi-Fan
  family: Chen
- given: Fei
  family: Sha
date: 2022-05-03
address:
container-title: Proceedings of The 25th International Conference on Artificial Intelligence
  and Statistics
volume: '151'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 3
pdf: https://proceedings.mlr.press/v151/arnold22a/arnold22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
