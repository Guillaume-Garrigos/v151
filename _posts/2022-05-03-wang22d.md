---
title: " The Tree Loss: Improving Generalization with Many Classes "
abstract: " Multi-class classification problems often have many semantically similar
  classes. For example, 90 of ImageNet’s 1000 classes are for different breeds of
  dog. We should expect that these semantically similar classes will have similar
  parameter vectors, but the standard cross entropy loss does not enforce this constraint.
  We introduce the tree loss as a drop-in replacement for the cross entropy loss.
  The tree loss re-parameterizes the parameter matrix in order to guarantee that semantically
  similar classes will have similar parameter vectors. Using simple properties of
  stochastic gradient descent, we show that the tree loss’s generalization error is
  asymptotically better than the cross entropy loss’s. We then validate these theoretical
  results on synthetic data, image data (CIFAR100, ImageNet), and text data (Twitter). "
software: " https://github.com/cora1021/TreeLoss "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang22d
month: 0
tex_title: " The Tree Loss: Improving Generalization with Many Classes "
firstpage: 6121
lastpage: 6133
page: 6121-6133
order: 6121
cycles: false
bibtex_author: Wang, Yujie and Izbicki, Mike
author:
- given: Yujie
  family: Wang
- given: Mike
  family: Izbicki
date: 2022-05-03
address:
container-title: Proceedings of The 25th International Conference on Artificial Intelligence
  and Statistics
volume: '151'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 3
pdf: https://proceedings.mlr.press/v151/wang22d/wang22d.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
