---
title: " An Information-Theoretic Justification for Model Pruning "
abstract: " We study the neural network (NN) compression problem, viewing the tension
  between the compression ratio and NN performance through the lens of rate-distortion
  theory. We choose a distortion metric that reflects the effect of NN compression
  on the model output and then derive the tradeoff between rate (compression ratio)
  and distortion. In addition to characterizing theoretical limits of NN compression,
  this formulation shows that pruning, implicitly or explicitly, must be a part of
  a good compression algorithm. This observation bridges a gap between parts of the
  literature pertaining to NN and data compression, respectively, providing insight
  into the empirical success of pruning for NN compression. Finally, we propose a
  novel pruning strategy derived from our information-theoretic formulation and show
  that it outperforms the relevant baselines on CIFAR-10 and ImageNet datasets. "
software: " https://github.com/BerivanIsik/SuRP "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: isik22a
month: 0
tex_title: " An Information-Theoretic Justification for Model Pruning "
firstpage: 3821
lastpage: 3846
page: 3821-3846
order: 3821
cycles: false
bibtex_author: Isik, Berivan and Weissman, Tsachy and No, Albert
author:
- given: Berivan
  family: Isik
- given: Tsachy
  family: Weissman
- given: Albert
  family: 'No'
date: 2022-05-03
address:
container-title: Proceedings of The 25th International Conference on Artificial Intelligence
  and Statistics
volume: '151'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 3
pdf: https://proceedings.mlr.press/v151/isik22a/isik22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
