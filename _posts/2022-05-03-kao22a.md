---
title: " Common Information based Approximate State Representations in Multi-Agent
  Reinforcement Learning "
abstract: ' Due to information asymmetry, finding optimal policies for Decentralized
  Partially Observable Markov Decision Processes (Dec-POMDPs) is hard with the complexity
  growing doubly exponentially in the horizon length. The challenge increases greatly
  in the multi-agent reinforcement learning (MARL) setting where the transition probabilities,
  observation kernel, and reward function are unknown. Here, we develop a general
  compression framework with approximate common and private state representations,
  based on which decentralized policies can be constructed. We derive the optimality
  gap of executing dynamic programming (DP) with the approximate states in terms of
  the approximation error parameters and the remaining time steps. When the compression
  is exact (no error), the resulting DP is equivalent to the one in existing work.
  Our general framework generalizes a number of methods proposed in the literature.
  The results shed light on designing practically useful deep-MARL network structures
  under the "centralized learning distributed execution" scheme. '
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kao22a
month: 0
tex_title: " Common Information based Approximate State Representations in Multi-Agent
  Reinforcement Learning "
firstpage: 6947
lastpage: 6967
page: 6947-6967
order: 6947
cycles: false
bibtex_author: Kao, Hsu and Subramanian, Vijay
author:
- given: Hsu
  family: Kao
- given: Vijay
  family: Subramanian
date: 2022-05-03
address:
container-title: Proceedings of The 25th International Conference on Artificial Intelligence
  and Statistics
volume: '151'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 3
pdf: https://proceedings.mlr.press/v151/kao22a/kao22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
