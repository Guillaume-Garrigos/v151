---
title: " Semi-Implicit Hybrid Gradient Methods with Application to Adversarial Robustness "
abstract: " Adversarial examples, crafted by adding imperceptible perturbations to
  natural inputs, can easily fool deep neural networks (DNNs). One of the most successful
  methods for training adversarially robust DNNs is solving a nonconvex-nonconcave
  minimax problem with an adversarial training (AT) algorithm. However, among the
  many AT algorithms, only Dynamic AT (DAT) and You Only Propagate Once (YOPO) is
  guaranteed to converge to a stationary point with rate O(1/K^{1/2}). In this work,
  we generalize the stochastic primal-dual hybrid gradient algorithm to develop semi-implicit
  hybrid gradient methods (SI-HGs) for finding stationary points of nonconvex-nonconcave
  minimax problems. SI-HGs have the convergence rate O(1/K), which improves upon the
  rate O(1/K^{1/2}) of DAT and YOPO. We devise a practical variant of SI-HGs, and
  show that it outperforms other AT algorithms in terms of convergence speed and robustness. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kim22c
month: 0
tex_title: " Semi-Implicit Hybrid Gradient Methods with Application to Adversarial
  Robustness "
firstpage: 4426
lastpage: 4445
page: 4426-4445
order: 4426
cycles: false
bibtex_author: Kim, Beomsu and Seo, Junghoon
author:
- given: Beomsu
  family: Kim
- given: Junghoon
  family: Seo
date: 2022-05-03
address:
container-title: Proceedings of The 25th International Conference on Artificial Intelligence
  and Statistics
volume: '151'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 3
pdf: https://proceedings.mlr.press/v151/kim22c/kim22c.pdf
extras:
- label: Supplementary ZIP
  link: https://proceedings.mlr.press/v151/kim22c/kim22c-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
