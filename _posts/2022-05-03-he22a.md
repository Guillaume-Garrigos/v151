---
title: " Near-optimal Policy Optimization Algorithms for Learning Adversarial Linear
  Mixture MDPs "
abstract: " Learning Markov decision processes (MDPs) in the presence of the adversary
  is a challenging problem in reinforcement learning (RL). In this paper, we study
  RL in episodic MDPs with adversarial reward and full information feedback, where
  the unknown transition probability function is a linear function of a given feature
  mapping, and the reward function can change arbitrarily episode by episode. We propose
  an optimistic policy optimization algorithm POWERS and show that it can achieve
  $\\tilde{O}(dH\\sqrt{T})$ regret, where $H$ is the length of the episode, $T$ is
  the number of interaction with the MDP, and $d$ is the dimension of the feature
  mapping. Furthermore, we also prove a matching lower bound of $\\tilde{\\Omega}(dH\\sqrt{T})$
  up to logarithmic factors. Our key technical contributions are two-fold: (1) a new
  value function estimator based on importance weighting; and (2) a tighter confidence
  set for the transition kernel. They together lead to the nearly minimax optimal
  regret. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: he22a
month: 0
tex_title: " Near-optimal Policy Optimization Algorithms for Learning Adversarial
  Linear Mixture MDPs "
firstpage: 4259
lastpage: 4280
page: 4259-4280
order: 4259
cycles: false
bibtex_author: He, Jiafan and Zhou, Dongruo and Gu, Quanquan
author:
- given: Jiafan
  family: He
- given: Dongruo
  family: Zhou
- given: Quanquan
  family: Gu
date: 2022-05-03
address:
container-title: Proceedings of The 25th International Conference on Artificial Intelligence
  and Statistics
volume: '151'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 3
pdf: https://proceedings.mlr.press/v151/he22a/he22a.pdf
extras:
- label: Supplementary ZIP
  link: https://proceedings.mlr.press/v151/he22a/he22a-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
