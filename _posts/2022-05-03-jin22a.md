---
title: " Federated Reinforcement Learning with Environment Heterogeneity "
abstract: " We study Federated Reinforcement Learning (FedRL) problem in which $n$
  agents collaboratively learn a single policy without sharing the trajectories they
  collected during agent-environment interaction. In this paper, we stress the constraint
  of environment heterogeneity, which means $n$ environments corresponding to these
  $n$ agents have different state-transitions. To obtain a value function or a policy
  function which optimizes the overall performance in all environments, we propose
  two algorithms, we propose two federated RL algorithms, QAvg and PAvg. We theoretically
  prove that these algorithms converge to suboptimal solutions, while such suboptimality
  depends on how heterogeneous these $n$ environments are. Moreover, we propose a
  heuristic that achieves personalization by embedding the $n$ environments into $n$
  vectors. The personalization heuristic not only improves the training but also allows
  for better generalization to new environments. "
software: " https://github.com/pengyang7881187/FedRL "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jin22a
month: 0
tex_title: " Federated Reinforcement Learning with Environment Heterogeneity "
firstpage: 18
lastpage: 37
page: 18-37
order: 18
cycles: false
bibtex_author: Jin, Hao and Peng, Yang and Yang, Wenhao and Wang, Shusen and Zhang,
  Zhihua
author:
- given: Hao
  family: Jin
- given: Yang
  family: Peng
- given: Wenhao
  family: Yang
- given: Shusen
  family: Wang
- given: Zhihua
  family: Zhang
date: 2022-05-03
address:
container-title: Proceedings of The 25th International Conference on Artificial Intelligence
  and Statistics
volume: '151'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 3
pdf: https://proceedings.mlr.press/v151/jin22a/jin22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
