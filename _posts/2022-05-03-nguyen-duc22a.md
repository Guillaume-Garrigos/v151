---
title: " Particle-based Adversarial Local Distribution Regularization "
abstract: " Adversarial training defense (ATD) and virtual adversarial training (VAT)
  are the two most effective methods to improve model robustness against attacks and
  model generalization. While ATD is usually applied in robust machine learning, VAT
  is used in semi-supervised learning and domain adaption. In this paper, we introduce
  a novel adversarial local distribution regularization. The adversarial local distribution
  is defined by a set of all adversarial examples within a ball constraint given a
  natural input. We illustrate this regularization is a general form of previous methods
  (e.g., PGD, TRADES, VAT and VADA). We conduct comprehensive experiments on MNIST,
  SVHN and CIFAR10 to illustrate that our method outperforms well-known methods such
  as PGD, TRADES and ADT in robust machine learning, VAT in semi-supervised learning
  and VADA in domain adaption. Our implementation is on Github: https://github.com/PotatoThanh/ALD-Regularization. "
software: " https://github.com/PotatoThanh/ALD-Regularization "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: nguyen-duc22a
month: 0
tex_title: " Particle-based Adversarial Local Distribution Regularization "
firstpage: 5212
lastpage: 5224
page: 5212-5224
order: 5212
cycles: false
bibtex_author: Nguyen-Duc, Thanh and Le, Trung and Zhao, He and Cai, Jianfei and Phung,
  Dinh
author:
- given: Thanh
  family: Nguyen-Duc
- given: Trung
  family: Le
- given: He
  family: Zhao
- given: Jianfei
  family: Cai
- given: Dinh
  family: Phung
date: 2022-05-03
address:
container-title: Proceedings of The 25th International Conference on Artificial Intelligence
  and Statistics
volume: '151'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 3
pdf: https://proceedings.mlr.press/v151/nguyen-duc22a/nguyen-duc22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
