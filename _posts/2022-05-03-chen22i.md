---
title: " Sample Complexity of Policy-Based Methods under Off-Policy Sampling and Linear
  Function Approximation "
abstract: " In this work, we study policy-based methods for solving the reinforcement
  learning problem, where off-policy sampling and linear function approximation are
  employed for policy evaluation, and various policy update rules (including natural
  policy gradient) are considered for policy improvement. To solve the policy evaluation
  sub-problem in the presence of the deadly triad, we propose a generic algorithm
  framework of multi-step TD-learning with generalized importance sampling ratios,
  which includes two specific algorithms: the $\\lambda$-averaged $Q$-trace and the
  two-sided $Q$-trace. The generic algorithm is single time-scale, has provable finite-sample
  guarantees, and overcomes the high variance issue in off-policy learning. As for
  the policy improvement, we provide a universal analysis that establishes geometric
  convergence of various policy update rules, which leads to an overall $\\Tilde{\\mathcal{O}}(\\epsilon^{-2})$
  sample complexity. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen22i
month: 0
tex_title: " Sample Complexity of Policy-Based Methods under Off-Policy Sampling and
  Linear Function Approximation "
firstpage: 11195
lastpage: 11214
page: 11195-11214
order: 11195
cycles: false
bibtex_author: Chen, Zaiwei and Theja Maguluri, Siva
author:
- given: Zaiwei
  family: Chen
- given: Siva
  family: Theja Maguluri
date: 2022-05-03
address:
container-title: Proceedings of The 25th International Conference on Artificial Intelligence
  and Statistics
volume: '151'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 3
pdf: https://proceedings.mlr.press/v151/chen22i/chen22i.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
