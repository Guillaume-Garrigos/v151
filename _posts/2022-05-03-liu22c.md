---
title: " Noise Regularizes Over-parameterized Rank One Matrix Recovery, Provably "
abstract: " We investigate the role of noise in optimization algorithms for learning
  over-parameterized models. Specifically, we consider the recovery of a rank one
  matrix $Y^*\\in R^{d\\times d}$ from a noisy observation $Y$ using an over-parameterization
  model. Specifically, we parameterize the rank one matrix $Y^*$ by $XX^\\top$, where
  $X\\in R^{d\\times d}$. We then show that under mild conditions, the estimator,
  obtained by the randomly perturbed gradient descent algorithm using the square loss
  function, attains a mean square error of $O(\\sigma^2/d)$, where $\\sigma^2$ is
  the variance of the observational noise. In contrast, the estimator obtained by
  gradient descent without random perturbation only attains a mean square error of
  $O(\\sigma^2)$. Our result partially justifies the implicit regularization effect
  of noise when learning over-parameterized models, and provides new understanding
  of training over-parameterized neural networks. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu22c
month: 0
tex_title: " Noise Regularizes Over-parameterized Rank One Matrix Recovery, Provably "
firstpage: 2784
lastpage: 2802
page: 2784-2802
order: 2784
cycles: false
bibtex_author: Liu, Tianyi and Li, Yan and Zhou, Enlu and Zhao, Tuo
author:
- given: Tianyi
  family: Liu
- given: Yan
  family: Li
- given: Enlu
  family: Zhou
- given: Tuo
  family: Zhao
date: 2022-05-03
address:
container-title: Proceedings of The 25th International Conference on Artificial Intelligence
  and Statistics
volume: '151'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 3
pdf: https://proceedings.mlr.press/v151/liu22c/liu22c.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
