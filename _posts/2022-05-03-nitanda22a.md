---
title: " Convex Analysis of the Mean Field Langevin Dynamics "
abstract: " As an example of the nonlinear Fokker-Planck equation, the mean field
  Langevin dynamics recently attracts attention due to its connection to (noisy) gradient
  descent on infinitely wide neural networks in the mean field regime, and hence the
  convergence property of the dynamics is of great theoretical interest. In this work,
  we give a concise and self-contained convergence rate analysis of the mean field
  Langevin dynamics with respect to the (regularized) objective function in both continuous
  and discrete time settings. The key ingredient of our proof is a proximal Gibbs
  distribution $p_q$ associated with the dynamics, which, in combination with techniques
  in Vempala and Wibisono (2019), allows us to develop a simple convergence theory
  parallel to classical results in convex optimization. Furthermore, we reveal that
  $p_q$ connects to the duality gap in the empirical risk minimization setting, which
  enables efficient empirical evaluation of the algorithm convergence. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: nitanda22a
month: 0
tex_title: " Convex Analysis of the Mean Field Langevin Dynamics "
firstpage: 9741
lastpage: 9757
page: 9741-9757
order: 9741
cycles: false
bibtex_author: Nitanda, Atsushi and Wu, Denny and Suzuki, Taiji
author:
- given: Atsushi
  family: Nitanda
- given: Denny
  family: Wu
- given: Taiji
  family: Suzuki
date: 2022-05-03
address:
container-title: Proceedings of The 25th International Conference on Artificial Intelligence
  and Statistics
volume: '151'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 3
pdf: https://proceedings.mlr.press/v151/nitanda22a/nitanda22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
