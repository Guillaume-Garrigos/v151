---
title: " Compressed Rule Ensemble Learning "
abstract: " Ensembles of decision rules extracted from tree ensembles, like RuleFit,
  promise a good trade-off between predictive performance and model simplicity. However,
  they are affected by competing interests: While a sufficiently large number of binary,
  non-smooth rules is necessary to fit smooth, well generalizing decision boundaries,
  a too high number of rules in the ensemble severely jeopardizes interpretability.
  As a way out of this dilemma, we propose to take an extra step in the rule extraction
  step and compress clusters of similar rules into ensemble rules. The outputs of
  the individual rules in each cluster are pooled to produce a single soft output,
  reflecting the original ensembleâ€™s marginal smoothing behaviour. The final model,
  that we call Compressed Rule Ensemble (CRE), fits a linear combination of ensemble
  rules. We empirically show that CRE is both sparse and accurate on various datasets,
  carrying over the ensemble behaviour while remaining interpretable. "
software: " https://github.com/maltenlz/Compressed-Rule-Ensembles "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: nalenz22a
month: 0
tex_title: " Compressed Rule Ensemble Learning "
firstpage: 9998
lastpage: 10014
page: 9998-10014
order: 9998
cycles: false
bibtex_author: Nalenz, Malte and Augustin, Thomas
author:
- given: Malte
  family: Nalenz
- given: Thomas
  family: Augustin
date: 2022-05-03
address:
container-title: Proceedings of The 25th International Conference on Artificial Intelligence
  and Statistics
volume: '151'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 3
pdf: https://proceedings.mlr.press/v151/nalenz22a/nalenz22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
