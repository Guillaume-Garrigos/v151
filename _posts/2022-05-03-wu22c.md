---
title: " Learning a Single Neuron for Non-monotonic Activation Functions "
abstract: " We study the problem of learning a single neuron $\\mathbf{x}\\mapsto
  \\sigma(\\mathbf{w}^T\\mathbf{x})$ with gradient descent (GD). All the existing
  positive results are limited to the case where $\\sigma$ is monotonic. However,
  it is recently observed that non-monotonic activation functions outperform the traditional
  monotonic ones in many applications. To fill this gap, we establish learnability
  without assuming monotonicity. Specifically, when the input distribution is the
  standard Gaussian, we show that mild conditions on $\\sigma$ (e.g., $\\sigma$ has
  a dominating linear part) are sufficient to guarantee the learnability in polynomial
  time and polynomial samples. Moreover, with a stronger assumption on the activation
  function, the condition of input distribution can be relaxed to a non-degeneracy
  of the marginal distribution. We remark that our conditions on $\\sigma$ are satisfied
  by practical non-monotonic activation functions, such as SiLU/Swish and GELU. We
  also discuss how our positive results are related to existing negative results on
  training two-layer neural networks. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wu22c
month: 0
tex_title: " Learning a Single Neuron for Non-monotonic Activation Functions "
firstpage: 4178
lastpage: 4197
page: 4178-4197
order: 4178
cycles: false
bibtex_author: Wu, Lei
author:
- given: Lei
  family: Wu
date: 2022-05-03
address:
container-title: Proceedings of The 25th International Conference on Artificial Intelligence
  and Statistics
volume: '151'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 3
pdf: https://proceedings.mlr.press/v151/wu22c/wu22c.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
