---
title: " On the Convergence Rate of Off-Policy Policy Optimization Methods with Density-Ratio
  Correction "
abstract: " In this paper, we study the convergence properties of off-policy policy
  optimization algorithms with state-action density ratio correction under function
  approximation setting, where the objective function is formulated as a max-max-min
  problem. We first clearly characterize the bias of the learning objective, and then
  present two strategies with finite-time convergence guarantees. In our first strategy,
  we propose an algorithm called P-SREDA with convergence rate $O(\\epsilon^{-3})$,
  whose dependency on $\\epsilon$ is optimal. Besides, in our second strategy, we
  design a new off-policy actor-critic style algorithm named O-SPIM. We prove that
  O-SPIM converges to a stationary point with total complexity $O(\\epsilon^{-4})$,
  which matches the convergence rate of some recent actor-critic algorithms in the
  on-policy setting. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: huang22a
month: 0
tex_title: " On the Convergence Rate of Off-Policy Policy Optimization Methods with
  Density-Ratio Correction "
firstpage: 2658
lastpage: 2705
page: 2658-2705
order: 2658
cycles: false
bibtex_author: Huang, Jiawei and Jiang, Nan
author:
- given: Jiawei
  family: Huang
- given: Nan
  family: Jiang
date: 2022-05-03
address:
container-title: Proceedings of The 25th International Conference on Artificial Intelligence
  and Statistics
volume: '151'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 3
pdf: https://proceedings.mlr.press/v151/huang22a/huang22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
