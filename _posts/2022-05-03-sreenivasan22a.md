---
title: " Finding Nearly Everything within Random Binary Networks "
abstract: " A recent work by Ramanujan et al. (2020) provides significant empirical
  evidence that sufficiently overparameterized, random neural networks contain untrained
  subnetworks that achieve state-of-the-art accuracy on several predictive tasks.
  A follow-up line of theoretical work provides justification of these findings by
  proving that slightly overparameterized neural networks, with commonly used continuous-valued
  random initializations can indeed be pruned to approximate any target network. In
  this work, we show that the amplitude of those random weights does not even matter.
  We prove that any target network of width $d$ and depth $l$ can be approximated
  up to arbitrary accuracy $\\varepsilon$ by simply pruning a random network of binary
  $\\{\\pm1\\}$ weights that is wider and deeper than the target network only by a
  polylogarithmic factor of $d, l$ and $\\varepsilon$. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sreenivasan22a
month: 0
tex_title: " Finding Nearly Everything within Random Binary Networks "
firstpage: 3531
lastpage: 3541
page: 3531-3541
order: 3531
cycles: false
bibtex_author: Sreenivasan, Kartik and Rajput, Shashank and Sohn, Jy-Yong and Papailiopoulos,
  Dimitris
author:
- given: Kartik
  family: Sreenivasan
- given: Shashank
  family: Rajput
- given: Jy-Yong
  family: Sohn
- given: Dimitris
  family: Papailiopoulos
date: 2022-05-03
address:
container-title: Proceedings of The 25th International Conference on Artificial Intelligence
  and Statistics
volume: '151'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 3
pdf: https://proceedings.mlr.press/v151/sreenivasan22a/sreenivasan22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
